{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code adpated from homework 2 of DS-GA 1012, source code:\n",
    "    https://colab.research.google.com/drive/1V_lMTW0m__CeSmMdntUoyriyyrQcAoUJ?authuser=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.6.2-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /anaconda3/lib/python3.7/site-packages (from imbalanced-learn) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.11 in /anaconda3/lib/python3.7/site-packages (from imbalanced-learn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.17 in /anaconda3/lib/python3.7/site-packages (from imbalanced-learn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /anaconda3/lib/python3.7/site-packages (from imbalanced-learn) (0.14.1)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.2\n"
     ]
    }
   ],
   "source": [
    "# check version number\n",
    "import imblearn\n",
    "print(imblearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "#!pip install sacremoses\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sacremoses\n",
    "from torch.utils.data import dataloader, Dataset\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "# change path if necessary\n",
    "train = pd.read_csv (r'/Users/christineshen/Desktop/NYU/Spring 2020/ML 1003/project/train.csv')\n",
    "dev = pd.read_csv (r'/Users/christineshen/Desktop/NYU/Spring 2020/ML 1003/project/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ex_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>prod_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>923</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-12-08</td>\n",
       "      <td>The food at snack is a selection of popular Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>924</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-16</td>\n",
       "      <td>This little place in Soho is wonderful. I had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>925</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>ordered lunch for 15 from Snack last Friday.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>926</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-07-28</td>\n",
       "      <td>This is a beautiful quaint little restaurant o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>927</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>Snack is great place for a  casual sit down lu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ex_id  user_id  prod_id  rating  label        date  \\\n",
       "0      0      923        0     3.0      1  2014-12-08   \n",
       "1      1      924        0     3.0      1  2013-05-16   \n",
       "2      2      925        0     4.0      1  2013-07-01   \n",
       "3      3      926        0     4.0      1  2011-07-28   \n",
       "4      4      927        0     4.0      1  2010-11-01   \n",
       "\n",
       "                                              review  \n",
       "0  The food at snack is a selection of popular Gr...  \n",
       "1  This little place in Soho is wonderful. I had ...  \n",
       "2  ordered lunch for 15 from Snack last Friday.  ...  \n",
       "3  This is a beautiful quaint little restaurant o...  \n",
       "4  Snack is great place for a  casual sit down lu...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check class imbalance in train set\n",
      "0    225055\n",
      "1     25819\n",
      "Name: label, dtype: int64\n",
      "check class imbalance in dev set\n",
      "0    32270\n",
      "1     3648\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check number of the 2 classes in train and dev \n",
    "print('check class imbalance in train set')\n",
    "print(train.label.value_counts())\n",
    "print('check class imbalance in dev set')\n",
    "print(dev.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X_train, X_dev, y_train, y_dev\n",
    "X_train, y_train = list(train.review),list(train.label)\n",
    "X_dev, y_dev = list(dev.review),list(dev.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-19 14:25:18--  https://docs.google.com/uc?id=1KMJTagaVD9hFHXFTPtNk0u2JjvNlyCAu\n",
      "Resolving docs.google.com (docs.google.com)... 2607:f8b0:400b:802::200e, 172.217.165.14\n",
      "Connecting to docs.google.com (docs.google.com)|2607:f8b0:400b:802::200e|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0k-0g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/21pho50n5gp9ai52bune1qru6u6832ug/1589912700000/14514704803973256873/*/1KMJTagaVD9hFHXFTPtNk0u2JjvNlyCAu [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-05-19 14:25:24--  https://doc-0k-0g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/21pho50n5gp9ai52bune1qru6u6832ug/1589912700000/14514704803973256873/*/1KMJTagaVD9hFHXFTPtNk0u2JjvNlyCAu\n",
      "Resolving doc-0k-0g-docs.googleusercontent.com (doc-0k-0g-docs.googleusercontent.com)... 2607:f8b0:400b:80f::2001, 172.217.164.193\n",
      "Connecting to doc-0k-0g-docs.googleusercontent.com (doc-0k-0g-docs.googleusercontent.com)|2607:f8b0:400b:80f::2001|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [audio/audible]\n",
      "Saving to: ‘glove_split.aa’\n",
      "\n",
      "glove_split.aa          [        <=>         ]  50.00M  10.3MB/s    in 5.1s    \n",
      "\n",
      "2020-05-19 14:25:32 (9.89 MB/s) - ‘glove_split.aa’ saved [52428800]\n",
      "\n",
      "--2020-05-19 14:25:32--  https://docs.google.com/uc?id=1LF2yD2jToXriyD-lsYA5hj03f7J3ZKaY\n",
      "Resolving docs.google.com (docs.google.com)... 2607:f8b0:400b:802::200e, 172.217.165.14\n",
      "Connecting to docs.google.com (docs.google.com)|2607:f8b0:400b:802::200e|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-08-0g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/inrac9dukcqr8n0s28d2e4u48elpto0t/1589912700000/14514704803973256873/*/1LF2yD2jToXriyD-lsYA5hj03f7J3ZKaY [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-05-19 14:25:38--  https://doc-08-0g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/inrac9dukcqr8n0s28d2e4u48elpto0t/1589912700000/14514704803973256873/*/1LF2yD2jToXriyD-lsYA5hj03f7J3ZKaY\n",
      "Resolving doc-08-0g-docs.googleusercontent.com (doc-08-0g-docs.googleusercontent.com)... 2607:f8b0:400b:80f::2001, 172.217.164.193\n",
      "Connecting to doc-08-0g-docs.googleusercontent.com (doc-08-0g-docs.googleusercontent.com)|2607:f8b0:400b:80f::2001|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘glove_split.ab’\n",
      "\n",
      "glove_split.ab          [           <=>      ]  50.00M  11.0MB/s    in 4.6s    \n",
      "\n",
      "2020-05-19 14:25:45 (10.8 MB/s) - ‘glove_split.ab’ saved [52428800]\n",
      "\n",
      "--2020-05-19 14:25:45--  https://docs.google.com/uc?id=1N1xnxkRyM5Gar7sv4d41alyTL92Iip3f\n",
      "Resolving docs.google.com (docs.google.com)... 2607:f8b0:400b:802::200e, 172.217.165.14\n",
      "Connecting to docs.google.com (docs.google.com)|2607:f8b0:400b:802::200e|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-04-0g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/eir225tpen4g854f29p06r5nv1621tdr/1589912700000/14514704803973256873/*/1N1xnxkRyM5Gar7sv4d41alyTL92Iip3f [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-05-19 14:25:48--  https://doc-04-0g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/eir225tpen4g854f29p06r5nv1621tdr/1589912700000/14514704803973256873/*/1N1xnxkRyM5Gar7sv4d41alyTL92Iip3f\n",
      "Resolving doc-04-0g-docs.googleusercontent.com (doc-04-0g-docs.googleusercontent.com)... 2607:f8b0:400b:802::2001, 172.217.165.1\n",
      "Connecting to doc-04-0g-docs.googleusercontent.com (doc-04-0g-docs.googleusercontent.com)|2607:f8b0:400b:802::2001|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/octet-stream]\n",
      "Saving to: ‘glove_split.ac’\n",
      "\n",
      "glove_split.ac          [           <=>      ]  23.49M  11.3MB/s    in 2.1s    \n",
      "\n",
      "2020-05-19 14:25:50 (11.3 MB/s) - ‘glove_split.ac’ saved [24629432]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Download Preprocessed version\n",
    "!wget https://docs.google.com/uc?id=1KMJTagaVD9hFHXFTPtNk0u2JjvNlyCAu -O glove_split.aa\n",
    "!wget https://docs.google.com/uc?id=1LF2yD2jToXriyD-lsYA5hj03f7J3ZKaY -O glove_split.ab\n",
    "!wget https://docs.google.com/uc?id=1N1xnxkRyM5Gar7sv4d41alyTL92Iip3f -O glove_split.ac\n",
    "!cat glove_split.?? > 'glove.6B.300d__50k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(glove_path, embedding_dim):\n",
    "    with open(glove_path) as f:\n",
    "        token_ls = [PAD_TOKEN, UNK_TOKEN]\n",
    "        embedding_ls = [np.zeros(embedding_dim), np.random.rand(embedding_dim)]\n",
    "        for line in f:\n",
    "            token, raw_embedding = line.split(maxsplit=1)\n",
    "            token_ls.append(token)\n",
    "            embedding = np.array([float(x) for x in raw_embedding.split()])\n",
    "            embedding_ls.append(embedding)\n",
    "        embeddings = np.array(embedding_ls)\n",
    "    return token_ls, embeddings\n",
    "\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "EMBEDDING_DIM=300 # dimension of Glove embeddings\n",
    "glove_path = \"glove.6B.300d__50k.txt\"\n",
    "vocab, embeddings = load_glove(glove_path, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98cc2f6adfd44e7bb160b7257484afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250874.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40dfdea148e644b48f52dc26d109f9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35918.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def featurize(data, labels, tokenizer, vocab, max_seq_length=128):\n",
    "    vocab_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    text_data = []\n",
    "    label_data = []\n",
    "    for ex in tqdm.tqdm_notebook(data):\n",
    "        tokenized = tokenizer.tokenize(ex.lower())\n",
    "        ids = [vocab_to_idx.get(token, 1) for token in tokenized]\n",
    "        text_data.append(ids)\n",
    "    return text_data, labels\n",
    "tokenizer = sacremoses.MosesTokenizer()\n",
    "train_data_indices, train_labels = featurize(X_train, y_train, tokenizer, vocab)\n",
    "val_data_indices, val_labels = featurize(X_dev, y_dev, tokenizer, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train text first 5 examples:\n",
      " [[2, 567, 24, 16202, 16, 9, 3694, 5, 816, 1624, 1, 2, 38945, 22187, 16, 221, 21, 16, 2, 1624, 1, 55, 37, 1, 19, 2, 446, 1, 65, 34, 21516, 7223, 189, 102, 22, 1, 1073, 607, 6, 171, 9930, 4], [39, 335, 243, 8, 19733, 16, 1, 43, 42, 9, 10240, 12613, 7, 9, 2849, 5, 1, 2, 628, 5639, 287, 12, 199, 359, 2, 1705, 17, 3, 36, 129, 380, 3, 39, 16, 1, 2, 823, 88, 32, 9, 335, 1, 7, 16693, 3, 36, 2, 567, 16, 355, 3, 122, 3318, 1, 1545, 42865, 272, 4], [1300, 4155, 12, 406, 27, 16202, 78, 1, 15, 81, 3, 938, 1512, 7, 2, 567, 17, 1, 43, 35, 297, 22, 6, 2, 1387, 130, 4155, 676, 3, 21, 1404, 3980, 46, 6860, 4], [39, 16, 9, 3368, 24612, 335, 2553, 15, 9, 1924, 1, 85, 83, 1, 29410, 133, 19733, 206, 24823, 3, 39, 56, 32, 9, 355, 243, 6, 839, 12, 9, 1, 43, 1437, 2, 3691, 61, 2, 10240, 12613, 3, 102, 43, 42, 6, 843, 1, 43, 1, 193, 1753, 14, 43, 1, 361, 594, 12684, 13, 27, 2, 5874, 6, 2, 12907, 7, 9079, 3, 2, 10240, 7, 2, 6894, 13, 17, 1, 160, 135, 1571, 34, 145, 9272, 115, 5, 2, 25111, 12613, 7, 355, 1, 43, 56, 7548, 39, 243, 6, 1546, 4], [16202, 16, 355, 243, 12, 9, 9413, 3164, 137, 1, 860, 15, 9, 1868, 1603, 1, 8, 111, 1422, 2, 2553, 12573, 2, 1, 2149, 555, 1, 2, 1, 8235, 17, 1, 16752, 7, 2, 6460, 11241, 9624, 24000, 555, 1, 2, 1624, 3426, 16, 496, 436, 41, 44459, 22, 19, 9814, 44, 909, 12, 31, 4003, 18669, 7, 3716, 1, 43, 3938, 396, 15, 224, 139, 12, 1, 85, 2, 359, 865, 1, 1, 2538, 83, 46, 2005, 2553, 16202, 1, 8, 2, 261, 635, 16, 122, 21, 221, 3, 85, 38, 441, 436, 7, 58, 1, 11575, 1, 19, 29123, 40, 161, 83, 1000, 145, 494, 759, 5, 4]]\n",
      "\n",
      "Train labels first 5 examples:\n",
      " [1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTrain text first 5 examples:\\n\", train_data_indices[:5])\n",
    "print(\"\\nTrain labels first 5 examples:\\n\", train_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list, max_sent_length=128):\n",
    "        \"\"\"\n",
    "        @param data_list: list of data tokens \n",
    "        @param target_list: list of data targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        self.max_sent_length = max_sent_length\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key, max_sent_length=None):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        if max_sent_length is None:\n",
    "            max_sent_length = self.max_sent_length\n",
    "        token_idx = self.data_list[key][:max_sent_length]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, label]\n",
    "\n",
    "    def spam_collate_func(self,batch):\n",
    "        \"\"\"\n",
    "        Customized function for DataLoader that dynamically pads the batch so that all \n",
    "        data have the same length\n",
    "        \"\"\" \n",
    "        data_list = [] # store padded sequences\n",
    "        label_list = self.target_list #??? label list doesnt change \n",
    "        # find length of longest sequence \n",
    "        seq_lengths = [len(i) for i in self.data_list]\n",
    "        max_batch_seq_len = max(seq_lengths) \n",
    "\n",
    "        if max_batch_seq_len >= self.max_sent_length:\n",
    "            max_batch_seq_len = self.max_sent_length \n",
    "\n",
    "        for seq in self.data_list: \n",
    "            #pad short sentences\n",
    "            if len(seq) < max_batch_seq_len:\n",
    "                seq += [int(0)] * (max_batch_seq_len - len(seq))\n",
    "            #trim long sentences\n",
    "            elif len(seq) > max_batch_seq_len:\n",
    "                seq = seq[:max_batch_seq_len]\n",
    "            else: pass\n",
    "\n",
    "            assert(len(seq) == max_batch_seq_len)\n",
    "            data_list.append(seq)\n",
    "        \n",
    "        assert(len(data_list) == len(label_list))\n",
    "        \n",
    "        #data_list = torch.Tensor(data_list)\n",
    "        #label_list = torch.Tensor(label_list)\n",
    "        \n",
    "        data_list = torch.from_numpy(np.array(data_list))\n",
    "        label_list = torch.LongTensor(label_list)\n",
    "        label_list = label_list.long()\n",
    "        \n",
    "        return [data_list, label_list]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "max_sent_length=128\n",
    "train_dataset = SpamDataset(train_data_indices, train_labels, max_sent_length)\n",
    "\n",
    "val_dataset = SpamDataset(val_data_indices, val_labels, train_dataset.max_sent_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=train_dataset.spam_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=train_dataset.spam_collate_func,\n",
    "                                           shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch dimension:  torch.Size([250874, 128])\n",
      "data_batch:  tensor([[   2,  567,   24,  ...,    0,    0,    0],\n",
      "        [  39,  335,  243,  ...,    0,    0,    0],\n",
      "        [1300, 4155,   12,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  88,    1,  205,  ...,    0,    0,    0],\n",
      "        [  42,    9,  355,  ...,    0,    0,    0],\n",
      "        [9390, 6743, 9538,  ...,  319,  183,   47]])\n",
      "labels:  tensor([1, 1, 1,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "data_batch, labels = next(iter(train_loader))\n",
    "\n",
    "print(\"data batch dimension: \", data_batch.size())\n",
    "print(\"data_batch: \", data_batch)\n",
    "print(\"labels: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,  567,   24,  ...,    0,    0,    0],\n",
       "        [  39,  335,  243,  ...,    0,    0,    0],\n",
       "        [1300, 4155,   12,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [  88,    1,  205,  ...,    0,    0,    0],\n",
       "        [  42,    9,  355,  ...,    0,    0,    0],\n",
       "        [9390, 6743, 9538,  ...,  319,  183,   47]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple BiLSTM Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTMClassifier classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, hidden_size, num_layers, num_classes, bidirectional, dropout_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = self.load_pretrained_embeddings(embeddings)\n",
    "        self.dropout = None\n",
    "        self.lstm = nn.LSTM(input_size=embeddings.shape[1],hidden_size=hidden_size,bidirectional=bidirectional,dropout=dropout_prob,batch_first=True,num_layers=num_layers)\n",
    "        self.non_linearity = nn.ReLU() # For example, ReLU\n",
    "        self.clf = nn.Linear(self.embedding_layer.embedding_dim,num_classes) # classifier layer\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \"\"\"\n",
    "           define the components of your BiLSTM Classifier model\n",
    "           You may refer to Lab2 for reference\n",
    "           2. TODO: Your code here\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "           The code for loading embeddings from Lab 2\n",
    "           Unlike lab, we are not setting `embedding_layer.weight.requires_grad = False`\n",
    "           because we want to finetune the embeddings on our data\n",
    "        \"\"\"\n",
    "        embedding_layer = nn.Embedding(embeddings.shape[0], embeddings.shape[1], padding_idx=0)\n",
    "        embedding_layer.weight.data = torch.Tensor(embeddings).float()\n",
    "        return embedding_layer\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        logits = None\n",
    "        \"\"\"\n",
    "           Write forward pass for LSTM\n",
    "           Example, forward:= embedding -> bilstm -> pooling (sum?mean?max?) \n",
    "                              nonlinearity -> classifier\n",
    "           Refer to: https://arxiv.org/abs/1705.02364 \n",
    "           Return logits\n",
    "           You may refer to Lab2 for embedding lookup and how to return logits\n",
    "           3. TODO: Your code here\n",
    "        \"\"\"\n",
    "        ## pass inputs to embedding layer\n",
    "        embedded = self.embedding_layer(inputs)\n",
    "\n",
    "\n",
    "        num_embedded = (inputs != self.embedding_layer.padding_idx) \\\n",
    "            .float().sum(dim=1).clamp(1)\n",
    "          \n",
    "        averaged_embeddings = embedded.sum(dim=1)/num_embedded.view(-1,1)\n",
    "        \n",
    "       ## max pooling => torch.max()\n",
    "        \n",
    "        #averaged_embeddings = self.non_linearity(averaged_embeddings)\n",
    "        out = self.clf(averaged_embeddings)\n",
    "        \n",
    "        \n",
    "        logits = self.sig(out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    accuracy = None\n",
    "    model.eval()\n",
    "    \"\"\"\n",
    "        4. TODO: Your code here\n",
    "        Calculate the accuracy of the model on the data in dataloader\n",
    "        You may refer to `run_inference` function from Lab2 \n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    #https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "    with torch.no_grad():\n",
    "        for data_batch, labels_batch in test_loader:\n",
    "            data_batch, labels_batch = data_batch.to(device), labels_batch.to(device)\n",
    "            output = model(data_batch)\n",
    "            test_loss += F.nll_loss(output, labels_batch, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(labels_batch.view_as(pred)).sum().item()\n",
    "            \n",
    "    accuracy = correct/len(test_loader.dataset)\n",
    "    #print(correct, len(test_loader.dataset))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize BiLSTM model, criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM hyperparameters\n",
    "hidden_size = 32\n",
    "num_layers = 3\n",
    "num_classes = 2\n",
    "bidirectional=True\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# if cuda exists, use cuda, else run on cpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "model = LSTMClassifier(embeddings, hidden_size, num_layers, num_classes, bidirectional)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14105a0d9ad040beb90f3311a183cdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_history = []\n",
    "val_accuracy_history = []\n",
    "best_val_accuracy = 0\n",
    "n_no_improve = 0\n",
    "early_stop_patience=2\n",
    "NUM_EPOCHS=10\n",
    "  \n",
    "for epoch in tqdm.tqdm_notebook(range(NUM_EPOCHS)):\n",
    "    model.train() # this enables regularization, which we don't currently have\n",
    "    for i, (data_batch, batch_labels) in enumerate(train_loader):\n",
    "        \"\"\"\n",
    "           Code for training lstm\n",
    "           Keep track of training of for each batch using train_loss_history\n",
    "        \"\"\"\n",
    "        preds = model(data_batch.to(device))\n",
    "        loss = criterion(preds, batch_labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss_history.append(loss.item())\n",
    "    # The end of a training epoch \n",
    "\n",
    "    \"\"\"\n",
    "        Code for tracking best validation accuracy, saving the best model, and early stopping\n",
    "        # Compute validation accuracy after each training epoch using `evaluate` function\n",
    "        # Keep track of validation accuracy in `val_accuracy_history`\n",
    "        # save model with best validation accuracy, hint: torch.save(model, 'best_model.pt')\n",
    "        # Early stopping: \n",
    "        # stop training if the validation accuracy does not improve for more than `early_stop_patience` runs\n",
    "        5. TODO: Your code here\n",
    "    \"\"\"\n",
    "    val_accuracy = evaluate(model,test_loader,device)\n",
    "    val_accuracy_history.append(val_accuracy)\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "      ## save as new best model\n",
    "        torch.save(model, 'best_model.pt')\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "        if n_no_improve > early_stop_patience:\n",
    "            print(\"Stopped at\",n_no_improve,\"iterations.\")\n",
    "            break  \n",
    "print(\"Best validation accuracy is: \", best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
